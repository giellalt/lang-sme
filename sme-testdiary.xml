<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE document PUBLIC "-//APACHE//DTD Documentation V2.0//EN"
"http://forrest.apache.org/dtd/document-v20.dtd">
<document>
  <header>
    <title>Test diary</title>
  </header>

  <body>
    <section id="Test_diary_for_the_Northern_Sámi_rule_and_lexicon_files">
      <title>Test diary for the Northern Sámi rule and lexicon files</title>

      <p>This document is not for error reports (they should be filed
      to our bug database <a
      href="http://giellatekno.uit.no/bugzilla/">Bugzilla</a>.
      Rather, what is found here is an overview of what has been
      tested, both vocabulary testing, testing of the disambiguator,
      and testing of the morphological analysis.</p>

      <section id="Test_planning">
        <title>Test plan</title>

        <p>Testing of text:</p>

        <p>Texts from various domains should be tested (the sme/corp/ mainly
        contains administrative texts (and the New Testament)):</p>

        <ul>
          <li>Fiction</li>
          <li>Religious texts</li>
          <li>Administrative language, politics, etc.</li>
          <li>Scientific texts</li>
        </ul>



        <p>Testing of the disambiguator:</p>

<p>Disambiguating is tested in the following way:</>

<ul>
<li> Ambiguity = #Parses / #Tokens</li>
<li> Recall = #Tokens Correctly Disambiguated / # Tokens = TP/P</li>
<li> Precision = #Tokens Correctly disambiguated / #Parses = TP/(TP+FP)</li>
</ul>

<p>A token is correctly disambiguated when <em>at least one</em> of
the readings (parses) of the token is correct.</p>.

<p>In the ideal case each token is uniquely and correctly
disambiguated with the correct parse. Here, both recall and precision
will be 1.0. A text where each token is annotated with all possible
parses, the recall will be 1.0, but the precision will be low. A high
recall thus comes with a price of low presicion. In other words: A
recall of less than 100% indicates that some correct analyses were
removed, and a precision of less than 100% indicates that some
wrong analyses were not removed. The goal is to have
both recall and precision as high as possible.</p>

<p>Testing procedure:</p>
<ol>

<li>Choose a reasonable short test, that has not been run on the
parser before</li> 

<li>Run the test
<ol>
<li>Run the text through the morphological parser, and inspect the
words that are not recognised by the parser. Add them to the parser,
or remove the sentence in question from the text. Eventually: just run
the test, but remove the sentences with unknown words afterwards. The
idea here is that it is unreasonable to demand disambiguation from a
sentence where some words are not recognised in the first place
</li>

<li>A different test would be to include all sentences, and just count
the failed words as well. They will have the analysis '?+', which of
course is wrong.</li> 
</ol></li>

<li>Count the ambiguity, prior to disambiguation. The number of parses
is found like this: <code>cat file.txt | preprocess
--abbr=bin/abbr.txt | lookup -flags mbTT -utf8 bin/sme.fst | lookup2cg
| egrep '\t\"' | wc -l</code>. The number of tokens is <code>cat
file.txt | preprocess --abbr=bin/abbr.txt | wc -l</code></li>

<li>Count the tokens correctly disambiguated (by hand): Read through
the analysed text, and count the number of words that have got a
correct analysis. The string is <code>cat file.txt | preprocess
--abbr=bin/abbr.txt | lookup -flags mbTT -utf8 bin/sme.fst | lookup2cg
| vislcg --grammar=src/sme-dis.rle | less</code>.</li>

<li>Then divide this number with the number of parses and get the
presicion, and with the number of tokens, and get the recall.</li>
</ol>

<p>During parser construction the recall and presicion data need not
be a goal in themselves. Another, equally important goal is to
identify errors and try to correct them. Deleting correct readings is
a more serious error than leaving the token ambigous.</p>




        <p>Grammatical testing:</p>

<p>We want to test the following:</p>

        <ul>
          <li>All forms of all paradigms</li>
          <li>All consonant gradation patterns</li>
          <li>All vowel alternations</li>
          <li>Compounds</li>
        </ul>

<p>There is a discussion of this on the newsgroup. TODO: copy that discussion into this document.</p>

      </section>

      <section id="Test_results">
        <title>Test results</title>

        <section id="Testing_the_parser_on_various_texts">
          <title>Vocabualry testing</title>

          <p>The following table records recall for word forms in
          various texts. Here we measure coverage of the vocabulary,
          by recording all word forms that are not recognised.</p>


          <source>---------------------------------------------------
nisson_ovddasteapmi.txt
Test 6  Wftot Wf-tkn  %-recall Tytot  Wf-typ %-recall
040903  38360  35704  93.0 %   20660  19102 92.4 %
---------------------------------------------------
hjh-nod1iid.txt
Test 5  Wftot Wf-tkn  %-recall Tytot  Wf-typ %-recall
040903   1580   1532  96.7 %     683   636  93.1 %
---------------------------------------------------
sd-divas-2002-{1,2}.txt
Test 4  Wftot Wf-tkn  %-recall Tytot  Wf-typ %-recall
041005  32835  31834  96.9 %    6664  6054  90.8 %
040913  32883  31255  95.0 %    6759  5856  86.6 %
---------------------------------------------------
sd-divas-2001-1.txt
Test 4  Wftot Wf-tkn  %-recall Tytot  Wf-typ %-recall
040903  60522  58549  96.7 %    8610  7610  88.4 %
040329  62459  60159  95.3 %    8496  7406  87.2 %
---------------------------------------------------
handlingsplan_samisk.txt
Test 3  Wftot Wf-tkn  %-recall Tytot  Wf-typ %-recall
031120   2148   2053  95,6 %    1044   984  94.3 %
040329   2461   2389  97.1 %     955   898  94.0 % (new preprocessor)
---------------------------------------------------
Test 2  Wftot Wf-tkn  %-recall Tytot  Wf-typ %-recall
Collection    225355                 32467 (test closed)
020815        203080  90.1 %         22721  70.0 %
020918        204315  90.7 %         22956  70.7 %
030210 227062~214845  94.6 %   31474~24398  77.5 %
---------------------------------------------------
Test 1  Wftot Wf-tkn  %-recall    Wf-types %-recall
New Testament 139681                 14888 (test closed)
011110         36471  26.1 %          4983  33.5 %
011116         36980  26.5 %          5050  33.9 %
011214         37736  27.0 %          5177  34.8 %
011218         40741  29.2 %          5955  40.0 % (closed classes added)
020129        126765  90.6 %         11676  78.4 % (proper names added)
020205        128702  92.1 %         12340  82.9 %
020206        129857  92.9 %         12328  82.8 % (nom_nom compound)
020207        131846  94.4 %         12500  84.0 %
020212        132394  94.8 %         12621  84.8 %
020213        132878  95.1 %         12652  85.0 %
020217        132993  95.2 %         12674  85.1 %
020306        133791  95.8 %         12850  86.4 %
020307        133821  95.8 %         12878  86.5 %
020318        134042  95.9 %         12914  86.7 %
020321        135446  97.0 %         13292  89.3 %
020323        136120  97.5 %         13373  89.8 %
020404        136621  97.8 %         13524  90.8 %
020410        136974  98.1 %         13609  91.4 %
020417        137435  98.4 %         13762  92.4 %
020418        137977  98.8 %         13875  93.2 %
020423        138101  98.9 %         13964  93.8 %
021104        138254  99.0 %         14003  94.1 %
040216 166194 165330  99.5 %  14916  14298  95.9 %
---------------------------------------------------
</source>
        </section>

        <section id="Explaining_the_table">
          <title>Explaining the table</title>

          <p>Lower token than type percentage indicates that the parser misses
          common words more often than seldom ones.</p>

          <p>Each text is given a separate section in the table, ordered
          chronologically, with the oldest test case (Test 1) at the bottom.
          The first line of each section gives the name of the file (note: the
          files of the test cases 2 and 3 are so changed that these two test
          cases are closed). Each line represents a test run. The first colum
          gives the test date (in the format ddmmyy), the second (WFtot) the
          total number of words in the file question, the third (Wf-tkn) the
          number of recognised word form tokens, and the percentage compared
          to the total. The next columns does the same for wordform types (cf.
          below for the commands used to calculate the numbers).</p>

          <source>-------------------------------------------------------------------------
Wftot:
cat filename | preprocess --abbr=bin/abbr.txt | wc -l

Non_recognised_wf:
cat filename | preprocess --abbr=bin/abbr.txt | lookup -flags mbTT -utf8 bin/sme.fst
 | grep '\?' | grep -v CLB | wc -l

Wf-tkn = Wftot - Non_recognised_wf

%-recall = Wf-tkn * 100 / Wftot
-------------------------------------------------------------------------
Tytot (Total number of wordform types):
cat filename | preprocess --abbr=bin/abbr.txt | sort | uniq | wc -l

Non_recognised_wt (Number of non-analysed wordform types:
cat filename | preprocess --abbr=bin/abbr.txt | sort | uniq |
lookup -flags mbTT -utf8 bin/sme.fst | grep '\?' | grep -v CLB | wc -l

Wf-typ (Number of recognised wordform types)
Wf-typ = Tytot - Non_recognised_wt

%-recall = Wf-typ * 100 / Tytot
--------------------------------------------------------------------------
</source>
        </section>
      </section>

      <section id="Grammatical_testing">
        <title>Grammatical testing</title>

        <p>February 2005: Here we will fill in an overview of which
        grammatical paradigms and wich sublexica we have tested.</p>

        <section>
          <title>Part of speech testing</title>

          <section>
            <title>Adjectives</title>

            <p>So far: ...</p>
          </section>

          <section>
            <title>Nouns</title>

            <p>lksfddf</p>
          </section>

          <section>
            <title>Verbs</title>

            <p>sdfjglgfæcføasjdfhlø</p>
          </section>
        </section>
      </section>
    </section>
  </body>
</document>
