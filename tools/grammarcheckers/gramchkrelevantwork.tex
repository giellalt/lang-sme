\documentclass[a4paper,english,12pt]{article}
\usepackage{babel}
\usepackage{ucs} %sami letters
% \usepackage{amssymb} %mathematical
\usepackage[utf8x]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{harvard}

\usepackage[dvips]{graphicx}
\usepackage{rotating} 

%\usepackage{tikz}
%\usepackage{array}
%\usepackage{arydshln} %has to be after array
%\usepackage{multirow}
\usepackage{graphics}
\usepackage{graphicx}
\usepackage{tabularx} %specified width
\usepackage{tipa}
\usepackage{booktabs}
\usepackage{ctable} %loads booktable by default
\usepackage{colortbl}
\usepackage{covington}
\usepackage{url}
\usepackage{harvard}
\usepackage[right=2.5cm,left=2.5cm,top=2cm,bottom=2cm]{geometry}
% \usepackage{bibtexlogo}
\usepackage{setspace}

\usepackage{fancyhdr}
\usepackage{linguex}

\pagestyle{fancy}
%\fancyfoot[LO,LE]{\slshape Rule-based Machine Translation from North to Lule Sámi}


\begin{document}

\setcounter{secnumdepth}{3}
\setcounter{tocdepth}{3}
%\linespread{1.5}
\begin{spacing}{1.0}


\newcommand{\tx}{\mbox{t\hspace{-.35em}-}} % for Sm
\newcommand{\txx}{\mbox{T\hspace{-.5em}-}} 




\title{{\Large Grammar checking - strategy}}


\author{Linda Wiechetek \\
		Universitetet i Tromsø \\
			(Norway)}
\date{}
\maketitle

\thispagestyle{empty}
\tableofcontents 
\thispagestyle{empty} %has to be after \maketitle

\newpage

\setcounter{page}{1} %in order to start pagenumbering

\parindent = 0mm
\parskip = 12pt

\section{Targetgroups}

\begin{itemize}
\item primary school children (Grammar errors are more frequent for this group of writers than for adults and the distribution of the error types is different in children’s texts) \cite{Sofkova2003}
\item dyslexic users \cite{Pedler2007} and \textit{OrdRet}
\item language learners
\item professional writers (Scarrie)
\item non-native speakers, e.g. Japanese English-speakers \cite{}
\end{itemize}


\section{Lingsoft's Grammarcheckers}

\begin{itemize}
\item Finnish (\url{http://www.lingsoft.fi/print.php?lang=en&doc_id=458})
\item Danish (\url{http://www.lingsoft.fi/dangrc/errors})
\item Swedish (\url{http://www.lingsoft.fi/swegrc/errors})
\item Norwegian (\url{http://www.lingsoft.fi/nobgrc/errors})
\end{itemize}

\subsection{Grammar errors treated}


\begin{itemize}
\item 40 error categories
\item distinguish between writing convention and grammar errors
\item 
\item 
\end{itemize}

\subsubsection{Writing convention errors}
\begin{itemize}
    \item Several spaces in a row
    \item Spaces in conjunction with quotation marks, parentheses, punctuation marks, special characters, such as %, §, © and °
	\item Correct punctuation (Hyphens, Dashes, Quotations, Parentheses, Multiple emphasis, Multiple punctuation marks, Ellipses 
	\item Parity of parentheses, brackets, braces
    \item Uneven number of quotation marks 
    \item Recognition and correction of certain abbreviations not handled by the spell checker
    \item Formatting of numbers (Phone numbers, Date expressions, long ordinal and decimal numbers)
    \item Sentence beginning with a lowercase letter
    \item Interrogative sentence beginning with a question word ending in full stop 
\end{itemize}

\subsubsection{Grammar errors (Finnish)}
\begin{itemize}
    \item No main clause
    \item No finite verb
    \item Number of finite verbs
    \item Sentence beginning with a coordinating conjunction 
	\item Agreement (subject-predicate, NP-internal number and case)
    \item Negative forms (periphrastic structures)
    \item Tense (periphrastic structures)
    \item Verb chains
    \item Double passive 
	\item Comparison: mitä/kaikkein + superlative, yhä + comparative, Comparative + kuin, mahdollisimman + positive, Not enemmän + positive, Not kaikista + superlative 
	\item Compounding (standardized compounds erroneously written as two words, standardized two word phrases erroneously compounded) 
 	\item Unnecessary repetition of clitics
    \item myös + -kin
    \item ja + ei
    \item Multiple sequential subordinate clauses
    \item Illegal endings attached to numbers
    \item Stylistically marked words
\end{itemize}

\subsubsection{Grammar errors (Swedish)}

\begin{itemize}
    \item Noun phrase (Definiteness form of noun/adjective)
    \item Number agreement: determiner and noun, adjective
    \item Gender agreement: determiner and noun, adjective and noun, pronoun and noun 
    \item Masculine form of adjective %    Note: This is a check for the masculine e-form of adjectives in front of unambiguous neuter nouns and some common (semantically) feminine nouns.
	\item	 Supine without "ha",    Double supine 
	\item    Double passive,    S-passive after certain verbs   
	\item  Infinitive after preposition, Infinitive without/with "att"
	\item    Number of finite verbs, No verb, No finite verb
	\item   Word order:  Position of adverb in subordinate clauses, negated element in subordinate clauses, Constituent order in subordinate interrogative clauses
	\item    Double negation
	\item     Subject complement agreement
	\item    Use of preposition with two-part conjunctions 
	\item    The construction ”möjligast” + adjective
\end{itemize}

\subsubsection{Grammar errors (Norwegian)}


\begin{itemize}
    \item     Definiteness form of noun conforming to definiteness of determiner
    \item Definiteness form of adjective when preceded by determiner and followed by noun
    \item Gender agreement: determiner and noun, adjective and noun when preceded by determiner
    \item Number agreement: determiner and noun, adjective and noun when preceded by determiner
    \item Infinitive without å
    \item Unambiguous infinitive requiring å
    \item Modal auxiliary verb agreement
    \item Past participle without ha
    \item Double s-passive
    \item S-passive after certain verbs
    \item Number of adjacent unambiguous finite verbs
    \item Lack of verb in sentence
	\item Word order: Position of clausal adverbs in subordinate clauses, Position of subject in simple clauses with initial adverb (inversion)
	\item Possessives in postposition: Definiteness form of noun, Gender agreement: noun and possessive, Number agreement: noun and possessive
    \item Agreement between subject and subject-complement: The check for non-agreeing subject complements is restricted to sentence-initial main clauses with the copula være
    \item Negative polarity items: Warning and suggestion mechanism regarding correct uses of ingen and noen as determiner in positive and negative sentences
    \item Gender agreement: quantifying pronoun and av-phrase
    \item Superfluous reflexive pronoun
    \item Pronoun form after preposition
\end{itemize}

\subsubsection{Grammar errors (Danish)}

\begin{itemize}
    \item Noun phrase: Definiteness: Double determination
    \item Definite form of noun after indeterminate determiners and plain adjectives
    \item Definite form of adjective after definite determiners
    \item Gender agreement: Determiner and noun, Adjective and noun, Pronoun and noun (certain pronouns like en/et, hver/hvert in front of an af + plural noun phrase)
    \item Number agreement: Determiner and noun, Adjective and noun
	\item    Infinitive with/without at
    \item Double -s passive
    \item Modal auxiliary verb agreement
    \item Past participle without have
    \item Supine vs. past participle after auxiliary verbs
    \item Number of adjacent unambiguous finite verbs
    \item Lack of verb in sentence
	\item Correct adjective gradation
	\item Word order: Position of clausal adverbs in subordinate clauses, Position of subject in simple clauses with clause initial adverb (inversion), Inversion in subordinate clauses, Postposed possessives/genitives, Position of negative objects
	\item Subject - subject complement agreement gender/number-wise in simple copula clauses
    \item Oblique form of personal pronouns when governed by a preposition
    \item nogen vs. nogle
    \item heller vs. hellere
\end{itemize}

\section{Other CG grammar checkers}

\begin{itemize}
\item Catalan system: makes use of CATCG, a modular general-purpose shallow morphosyntactic parser for unrestricted Catalan text (Badia et al., 2001), several error detection modules (CatSpel - dealing with non-words; CG-based morphological error detection module: simple orthographic errors resulting in words such as wrong use of apostrophe, or wrong NP-agreement; CG-based negative n-gram error detection module: wrong word sequences; CG-based morphosyntactic error detection module: subject-verb disagreement, subcategorization errors, etc.; domain-specific modules: certain semantic or pragmatic errors), CATCG’s morphological disambiguation grammar and syntactic mapping module has been modified in such a way that word sequences that otherwise would imply a wrong or simply a non disambiguation are now analysed as a possible reading.
%Up to now only bigrams and trigrams have been used (also implemented
%using CG-based grammar). They have been collected
%via a semi-automatic procedure of detection errors
%in a POS-tagged corpus similar to the one used by (Kveton
%and Oliva, 2002).
\item \textit{Lingvohelpilo} Esperanto language helper (grammar checker with pedagogical advice): CG-parser EspGram (accuracy rate (F-score) of 92.1\% for syntactic parsing and dependency on ordinary Esperanto texts with few errors), CG3 based including dependencies, Esperanto users on an intermediate or high level of proficiency or, more precisely, those who do not have an error density higher than 1 error / 10 words.
\item
\end{itemize}

\section{Non CG grammar checkers}

\begin{itemize}
\item Scarcheck was initiated in 1996 there was no grammar checker for Swedish (valency errors: No finite verb, "att" missing after the verb "komma" and others, "att" missing after preposition, "att" doubled, "att" redundant)
\item \cite{Carlberger2004} GRANSKA: compound errors, internal NP disagreement errors, subject-predicate disagreement (involves long distance dependencies) and more
\item As well as in English (with mostly open compounds), Swedish compounds cannot
be listed – compounds are the most frequent hapax words – and a lexical approach
is thus out of question. The semantics of a compound normally constructs one
obvious interpretation, and one more rare. The last part of the compound gives the
word its word class, but not necessary its semantic interpretation. The relations
between the lexemes in the compound are complex.
\item
\end{itemize}

\section{Grammar checkers for Fenno-Ugric languages}

\begin{itemize}
\item \textit{VIRKKU} (Finnish) undocumented\footnote{Cf. \cite{Arppe2000}: Virkku was developed and launched on the market in 1991 by Kielikone Ltd <http://www.kielikone.fi> as a side-kick of the company’s long-term efforts in developing a machine translation system from Finnish to English. Despite this technical background, Virkku does not use the full-scale deep-syntactic parser developed for Kielikone’s machine translation system, but is instead based on a lighter, unification- based approach. Unfortunately, the Virkku system remains publicly undocumented. } 
\item Lingsoft \textit{FINGRC} (Finnish): basic grammar and style checking\footnote{cf. \url{http://www.lingsoft.fi/en/458}} for MSOffice \footnote{cf. \url{http://www.lingsoft.fi/fingrc_errors}} 
\item \textit{Voikko} (Finnish) spell and grammar checking,  hyphenator for LibreOffice and OpenOffice?
\item Lightproof (Hungarian) \footnote{\url{https://apps.ubuntu.com/cat/applications/quantal/libreoffice-lightproof-hu/} and \url{http://extensions.openoffice.org/en/project/lightproof-huhu}} for LibreOffice/OpenOffice 
\end{itemize}


\section{Error types and error type distinctions}

\subsection{Causes of errors}

\begin{itemize}
\item the real cause of an error is not always clear from the text \cite[p.27]{Sofkova2003}
\item 
\end{itemize}

\subsection{Error type distinctions}

\begin{itemize}
\item 1) Errors caused by a sloppy use of the "cut and paste" options of modern text editors, and 2) errors due to people's mistaken beliefs about the written language norms. \cite{Hagen2001a}
\item typographical errors (typing errors or OCR scanning errors), orthographical errors (erroneous transliterations of phonemes to graphemes) and, most importantly, morpho-syntactic errors (resulting from misapplication of morphological inflection and syntactic rules) \cite{Vosse1992}
\item Grammar errors violate (mostly) the syntactic rules of a language, such as feature agreement, order or choice of constituents in a phrase or sentence, thus concerning a wider context than a single word \cite[p.27]{Sofkova2003}
\item typographical error vs. competence error
\item whether the error is realized as a non-word or a real word:
	\begin{itemize}
	\item All real word errors, that violate a syntactic rule and result in other
	forms of the same lemma are classified as grammar errors.
	\item All real word errors resulting in new lemmas by a change of the
	whole word are classified as grammar errors.
	\item All real word errors resulting in new lemmas by a change in (one
	or more) letter(s) are classified as spelling errors. \cite[p.30]{Sofkova2003}
	\end{itemize}
\end{itemize}

\subsection{Error types}

\begin{itemize}
\item Notoriously difficult to spot are word doubling errors, especially at the end of a line \cite{Vosse1992}: många som mobbar har har det oftast dåligt hemma \cite[p.64]{Sofkova2003}
\item errors in idiomatic expressions (some words/forms occurr only in idiomatic expressions)
\item compound conventions
\item comma errors
\item Parity of parentheses, of brackets, of braces (FINGRC)
\item  Uneven number of quotation marks (FINGRC)
\item Errors caused by a sloppy use of the "cut and paste" options of modern text editors
\item agreement errors (typical syntactic errors) \cite{Vosse1992}
\begin{itemize}
\item NP-internal agreement: disagree according to gender, number and definiteness
\item agreement errors appear between the subject and the predicative, which can involve long distance dependencies
\item anaphoric agreements between phrases within the sentence or between sentences
\end{itemize}
\item homophony: words having the same pronunciation but a different spelling, A special case of homophonous words are words which differ only in inflection.
\item A real-word spelling error occurs when one word is mistakenly produced for another, such as there for their \cite{Pedler2005}, identify sets (usually pairs) of words that are likely to be confused, such as dairy and diary. If the words in the set differ in their parts-of-speech (board, bored), this decision can be based on syntax, but if the parts-of-speech are identical, some use of semantics is required, test frequency in the corpus
\end{itemize}

\subsection{Realword errors}

\begin{itemize}
\item confusion sets of words that differ from each other by a single letter (insertion, omission, substitution or transposition) \cite{Pedler2007}
\item trigrams - Many of the programs achieve an accuracy of over 90\% while highly developed applications
such as SNoW achieve over 95\% on average (Carlson et al., 2001). \cite{Pedler2007}
\item confusion sets: \cite{Pedler2007} uses 1940 pairs of words with matching tagsets and 3606 pairs with some but not all tags in common
\item The probability of each hypernym occurring in the vicinity of a particular confusable is calculated by dividing its word count by the total word count for the tree − i.e. the count stored in the root node of the tree (1714 in the case of carve).
\item When the spellchecker comes across a confusable word, it needs to decide whether the word it has seen is more likely in the context than one of the other members of its confusion set. For example, given the confusable pair {carve, crave}, when it encounters carve, it needs to decide whether this is indeed the word the user intended or whether crave would be a more appropriate choice.
\item ‘lexeme-given-hypernym’ probability: weight each of the ‘hypernym-given-lexeme’ probabilities in the lexeme trees by the relative frequency with which each hypernym-lexeme pair for the confusion set − {carve, crave} in this case − occurred with the confusable in the BNC. We can then divide these ‘weighted probabilities’ by the overall probability for the hypernym occurring with either confusable to obtain a ‘relative likelihood’ score for each confusable lexeme − this will be a value between 0 and 1 for each confusable such that the sum of the scores is 1. Equation 1 below is used to calculate the weighted probablilities followed by equation 2 to calculate the final relative likelihood scores.

%When the program encounters one of the semantic confusables in the text, it first checks for cooccurring nouns (words that had been assigned a noun tag by the syntax checker). If there are none it will be unable to make a decision and so will simply continue through the text until it finds
%another semantic confusable. The hypernym trees were based on nouns that had co-occurred in a
%window size of +/- 2 words from the corresponding confusable in the BNC, but a large proportion
%of the confusables to be checked may not have a co-occurring noun in this vicinity. For instance,
%only 41% of the confusables in the FLOB corpus used for testing the initial program had a cooccurring
%noun within +/- 2 words. Increasing the size of the window that the spellchecker uses to
%retrieve co-occurring nouns means that it is able to check a larger number of confusables; there
%were noun co-occurrences for 97% of the confusables in the FLOB corpus when the window size
%was increased to +/- 10. Based on this finding, to maximize the number of confusable occurrences
%that the program could attempt to correct, the program has been set to check for co-occurring nouns
%within a window of ten words each side of a confusable (unless it encounters a sentence boundary
%first). However, the further away a noun is, the less likely it is to be related to the confusable so the
%scores retrieved for each noun are reduced as their distance from the confusable increases. (The scoring process is described in detail in Section 9.5.4 below.)
%Similarly, although each word in a confusion set is considered
%confusable with the headword − pat, pet and pith are all confusable with pit − the members of a
%confusion set are not necessarily all confusable with each other. The confusion set for pet, for
%example, does not include pith, whereas the confusion set for pith contains path in addition to pit.
\end{itemize}


\section{Precision and Recall}

\begin{itemize}
\item Precision: good alarms divided by the sum of good alarms and false alarms
\item Swedish grammar checker (see Birn 2000, Arppe 2000): resulting precision 70\% (according to Birn 2000:38), counted as good alarms divided by the sum of good alarms and false alarms. 
\item Norwegian grammar checker is 75 \% precision from a test corpus of 890 000 words from the newspapers Nordlys and Sarbsborg Blad \cite{Hagen2001a} (However, depending on what kind of text that is used as a test corpus and what kind of rules that are included, these numbers can vary a great deal. For example, if we include a rule that tests whether there are any finite verbs in a sentence, and apply the grammar checker on newspaper text with a lot of verb-less headlines, the precision rises to approximately 91 \% (and the same goes for the Swedish grammar checker).
\item Prediction accuracy: is a measure of the program's ability to select the correct word from a set of alternatives in a given context expressed as the proportion of correct decisions out of all decisions made for confusable words in a text \cite{Pedler2007} 
\item Accuracy vs. coverage \cite[p.186]{Pedler2007}
\end{itemize}

\section{Relation spellchecker grammarchecker}

\begin{itemize}
\item Before entering the sentence level (i.e., parsing a sentence), a spelling module should check on all the words in the sentence. \cite{Vosse1992}
\item
\end{itemize}

\section{Resolving compound errors}

There are three problems involved
in compound analysis: (1) not every sequence of dictionary
words forms a legal compound, (2) certain
parts of a compound cannot be found in the dictionary
and (3) full analysis usually comes up with
too many alternatives. My solution follows the lines
set out in (Daelemans, 1987): a deterministic word
parser, constrained by the grammar for legal compounds,
that comes up with the left-most longest
solution first. This solution is rather fast on legal
compounds, while it takes at most O(n 2) time for
nonexistent words and illegal compounds. \cite{Vosse1992}

\section{Technical stuff}

\begin{itemize}
\item Firefox Add-on \cite{Petrovic2009}
\item interactive mode (Grammatifix)
\end{itemize}


\section{Good solutions}

\begin{itemize}
\item If a sentence has a great proportion of unknown words, it makes little sense to apply grammar and spelling checking rules to it, since it is probably a non-Swedish sentence. Instead, such a sentence is either ignored, marked as suspect in its entirety, or scrutinized anyway, according to the user’s preference. \cite{Carlberger2004}
\item
\end{itemize}

\bibliographystyle{jmr}
\bibliography{gramchkrelevantwork}


\end{document}